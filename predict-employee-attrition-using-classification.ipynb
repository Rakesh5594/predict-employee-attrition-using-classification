{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##**Scenario:** Predict Employee Attrition using Classification Algorithms\n","metadata":{"id":"hw3FkGzXpAym"}},{"cell_type":"markdown","source":"###**Dataset Description**\nThe data set contains the following attributes:\n\n- **satisfaction_level** \n- **last_evaluation**\n- **number_project**\n- **average_montly_hours**\n- **time_spend_company**\n- **Work_accident**\n- **quit**\n- **promotion_last_5years**\n- **department**\n- **salary**","metadata":{"_uuid":"8acee0a244846f03a1436fb98dc90f6a8a86fdbf","id":"MBjCgFbQyCDT"}},{"cell_type":"code","source":"  #Importing Required Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas_profiling import ProfileReport\nsns.set()\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")    \nimport os\n\nprint('Libraries Imported')","metadata":{"_cell_guid":"c73461da-4bef-4ea8-a0ab-3470d226570c","_uuid":"4014df0650f6999b691d3b516a6a62c3ac309073","id":"P8vaerdk_D0G","outputId":"f4bbd3dd-cca1-46a2-d39c-68256fd3b6a0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading the dataset\n\ndf = pd.read_csv('employee_data.csv?dl=0')\n\ndf.head() #Printing the first 5 rows of dataframe","metadata":{"_cell_guid":"aefd94fe-b2a8-46a6-8559-c52a4f3c32c5","_uuid":"525df8587d859504c7059639bbcfbaa3352c2635","id":"hqesLGGA_D0W","outputId":"96813279-9f18-4b8c-d27d-e5a836793f4e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###**Exploratory Data Analysis**","metadata":{"id":"390_8cteTkvf"}},{"cell_type":"markdown","source":"**Note:** If you want to learn more about Pandas-Profiling [**Click Here!**](https://pypi.org/project/pandas-profiling/)","metadata":{"id":"xsujqAS_vbjm"}},{"cell_type":"code","source":"import pandas_profiling\nfrom pandas_profiling import ProfileReport\nprof = ProfileReport(df)\nprof.to_file(output_file='output.html') #Generating a Data Report\n","metadata":{"id":"RdbFYXSGnzIJ","outputId":"48ea4042-46d8-4c1e-e492-29a0daaa57d7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ProfileReport(df).to_notebook_iframe()","metadata":{"id":"1alfG61zfj5I","outputId":"1a6107f3-d2d0-429b-e328-8c300165f238"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generating a Pandas Profiling Report \n\nimport pandas_profiling\nfrom pandas_profiling import ProfileReport\nprof = ProfileReport(df)\nprof.to_file(output_file='output.html')","metadata":{"id":"CBg79aWet7FM","outputId":"ca6fc3aa-97b8-451f-869e-77f5461d01fd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Please refer to the HTML file created by the name of **output.html**","metadata":{"id":"lgUZ5iO1UVro"}},{"cell_type":"markdown","source":"___\n**Observations:**\n\n- There are **10** variables or features in the dataframe and the total number of instances or rows are **14999**\n- We have **5** Numeric, **3** Boolean, and **2** Categorical variables\n- We have **2** Categorical Variables namely **Department** and **Salary**\n- The **Salary** column is divided into **low**, **medium**, and **high** \n- There are no missing cells in the dataset which is a big relief\n___","metadata":{"id":"VXRT7Ftyx0Lw"}},{"cell_type":"markdown","source":"","metadata":{"id":"16Cq3sDJmv1Z"}},{"cell_type":"code","source":"import plotly.express as px\nfig = px.histogram(df, x = 'average_montly_hours')\nfig.show()","metadata":{"id":"bQp2EcC4wlwY","outputId":"ef31afc7-db90-4287-ebf0-afbe48f9f913"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n**Observations:**\n- Most of the employees work between 125 and 265 hours monthly\n- Very few employees spend less than 140 and more than 265 working on a monthly basis\n___","metadata":{"id":"SddwM-57DbGi"}},{"cell_type":"code","source":"fig = px.histogram(df, x = 'satisfaction_level')\nfig.show()","metadata":{"id":"Qd0xYF5gC61c","outputId":"cd87ab69-8d6f-4236-95f6-95424e61fba8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n**Observations:**\n- More than 800 employees are not satasfied with their work and may leave the company\n- Most of the employees are quite content with their job\n___","metadata":{"id":"KsEr-BRaEHAS"}},{"cell_type":"markdown","source":"**What's the Attrition percentage in the company?**\n","metadata":{"id":"01Xeor1hL3R8"}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\n\nax = sns.countplot(df[\"quit\"], color='green')\nfor p in ax.patches:\n    x = p.get_bbox().get_points()[:,0]\n    \n    y = p.get_bbox().get_points()[1,1]\n    \n    ax.annotate('{:.2g}%'.format(100.*y/len(df)), (x.mean(), y), ha='center', va='bottom')\nplt.show()","metadata":{"id":"AJIm1KLi0in-","outputId":"58ac3aa9-3ff7-440a-af3f-a3d879a883dd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n**Observations:**\n- 76% of employees did not leave the organization while 24% did leave\n___","metadata":{"id":"bzGJyCiiFQtP"}},{"cell_type":"markdown","source":"**Which Department of the company has the highest Attrition rate?**","metadata":{"id":"dsww97i3GPbd"}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\n\nsns.countplot(data=df,x=df['department'],hue=\"quit\")\n\nplt.xlabel('Departments')\nplt.ylabel('Frequency')\n\nplt.show()","metadata":{"id":"isAjN3ckFb0j","outputId":"55d7e878-00bb-426e-feb4-057b5f370f4e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n**Observations:**\n- **Sales** department has the highest attrition or turnover rate followed by **technical**, and **support** departments\n- **Management** recorded the lowest number of employees leaving the company\n___","metadata":{"id":"_v-nsR2VFwFE"}},{"cell_type":"markdown","source":"####**Bi-variate Distributions**\n- A Bi-variate distribution is a distribution of two random variables\n- The concept generalizes to any number of random variables, giving a **Multivariate Distribution**","metadata":{"id":"8YOP7LrsUXMi"}},{"cell_type":"markdown","source":"**How does salary affect the attrition rate?**","metadata":{"id":"-jTPjFV4HccC"}},{"cell_type":"code","source":"df_new = pd.crosstab(df['salary'], df['quit'])\n\ndf_new.plot(kind = 'bar')\n\nplt.title('Employee Attrition Frequency based on Salary')\nplt.xlabel('Salary')\nplt.ylabel('Frequency')\n\nplt.show()","metadata":{"id":"Tn3OJPPC0h9H","outputId":"a960258f-6421-4a55-c350-836da14f55f8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n**Observations:**\n\n- People with **low** salary are more likely to quit as compared to people with **medium** and **high** salaries\n- People with **high** salary are very less likely to leave the organization\n- Salary seems to be a significant factor in determining the turnover rate in employees\n___","metadata":{"id":"6SeNtHv13OFp"}},{"cell_type":"markdown","source":"**Do experienced employees tend to leave the company if they are not satisfied?**","metadata":{"id":"XLhbUpUcMcQU"}},{"cell_type":"code","source":"px.scatter(df, x=df['satisfaction_level'],y=df['time_spend_company'],color=df['quit'])","metadata":{"id":"XhpoTHNOuuAY","outputId":"9f91c3d9-5846-4d34-e46d-2dfe1e7c1a72"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Which department executes the most number of projects?**","metadata":{"id":"o1aJAI9ZXkr1"}},{"cell_type":"code","source":"fig = px.box(df, x=\"department\",y=\"number_project\")\nfig.show()","metadata":{"id":"7slGAJ9DvWhw","outputId":"90ada93b-a3e1-48d0-de0f-db3bf244743c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###**Create Training and Testing Set**","metadata":{"id":"Px9uWu7z-_rv"}},{"cell_type":"code","source":"X = df.drop('quit', axis = 1)\ny = df.quit","metadata":{"id":"TQQpaVcf7XY0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size=0.2, stratify = y)","metadata":{"id":"AT9KdcJF7Xrv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###**Data Pre-processing**","metadata":{"id":"CAJEow0-Ud5V"}},{"cell_type":"markdown","source":"####**Encode Categorical Variables**\n\nThe dataset contains **2** Categorical Variables:\n\n- **department**\n- **salary**\n\nWe have to encode them before modelling because scikit learn doesn't accept string data as input","metadata":{"id":"RrxjSNkk5k25"}},{"cell_type":"code","source":"cat_vars = ['department', 'salary']\n\nfor vars in cat_vars:\n  cat_list = pd.get_dummies(X_train[vars], prefix=vars)\n  X_train = X_train.join(cat_list)","metadata":{"id":"Nd2YlvY7v5GX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_vars = ['department', 'salary']\n\nfor vars in cat_vars:\n  cat_list = pd.get_dummies(X_test[vars], prefix=vars)\n  X_test = X_test.join(cat_list)","metadata":{"id":"2QeaWPrJwEbg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let us drop the department and salary columns\n\nX_train.drop(columns=['department', 'salary'], axis = 1, inplace=True)\nX_train.shape","metadata":{"id":"HQof9L8n7WXE","outputId":"108eca6c-8bb1-49fe-c7ff-8107ba80663d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.drop(columns=['department', 'salary'], axis = 1, inplace=True)\nX_test.shape","metadata":{"id":"29wj2KBswqgg","outputId":"d57675f5-6a36-40b6-de4f-48d64fdfd082"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###**Build an Interactive Decision Tree Model**","metadata":{"id":"ll2Pa3tDBdMT"}},{"cell_type":"markdown","source":"[**Click Here!**](https://ipywidgets.readthedocs.io/en/latest/) to learn more about **ipywidgets**","metadata":{"id":"QnFnEogaEj69"}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree\nfrom sklearn.tree import export_graphviz # display the tree within a Jupyter notebook\nfrom IPython.display import SVG\nfrom graphviz import Source\nfrom IPython.display import display\nfrom ipywidgets import interactive, IntSlider, FloatSlider, interact\nimport ipywidgets\nfrom IPython.display import Image\nfrom subprocess import call\nimport matplotlib.image as mpimg","metadata":{"id":"EiorUo6y7Xgp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@interact #To convert any function into an inteactive one just write \"@interact\" immediately before the function definition\n\ndef plot_tree(\n    crit = ['gini', 'entropy'],\n    split = ['best','random'],\n    depth = IntSlider(min = 1, max = 25, value =2, continuous_update = False),\n    min_split = IntSlider(min = 1, max = 5, value =2, continuous_update = False),\n    #min_split is the minimum number of samples  required to split an internal node in our decision tree\n    min_leaf = IntSlider(min = 1, max = 5, value =1, continuous_update = False)):\n  \n  estimator = DecisionTreeClassifier(criterion=crit,\n                                     splitter=split,\n                                     max_depth = depth,\n                                     min_samples_split = min_split,\n                                     min_samples_leaf = min_leaf\n                                     )\n  estimator.fit(X_train, y_train)\n  print('Decision Tree Training Accuracy:', accuracy_score(y_train, estimator.predict(X_train)))\n  print('Decision Tree Testing Accuracy:', accuracy_score(y_test, estimator.predict(X_test)))\n\n  a = accuracy_score(y_train, estimator.predict(X_train))\n  b = accuracy_score(y_test, estimator.predict(X_test))\n\n  if a > 0.99:\n    print('Decision Tree Training Accuracy',a, 'Decision Tree Testing Accuracy', b)\n    print('Criterion:',crit,'\\n', 'Split:', split,'\\n', 'Depth:', depth,'\\n', 'Min_split:', min_split,'\\n', 'Min_leaf:', min_leaf,'\\n')\n\n  #Let us use GraphViz to export the model and display it as an image on the screen\n  graph = Source(tree.export_graphviz(estimator, out_file=None, \n                                      feature_names = X_train.columns,\n                                      class_names = ['stayed', 'quit'],\n                                      filled = True))\n  \n  display(Image(data=graph.pipe(format = 'png')))\n  ","metadata":{"id":"UVOccIsi7Xd3","outputId":"4fa14f82-2e67-4f3c-b651-4fd8e2ebd0a8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Advantages** & **Disadvantages** of Decision Tree:\n\n**Advantages:**\n- Interpretable and easy to understand\n- Can Handle Missing Values\n- Feature Selection happens automatically\n\n\n**Disadvantages:**\n- Prone to overfitting\n- Tends to add High Variance which means they tend to overfit\n- Small changes in data greatly affect prediction\n \n\n","metadata":{"id":"C59pnZD2PbcP"}},{"cell_type":"markdown","source":"**One problem with Decision Tree is that they have Low Bias and High Variance which means they are prone to overfitting on the training set**\n\n","metadata":{"id":"gXrkzhRJIAoj"}},{"cell_type":"markdown","source":"Now, let us see what **Underfit**, **Goodfit**, and **Overfit** is:\n\n- **Underfit**\n  - Model has not learned anything\n  - **Training Accuracy**: **54%**\n  - **Testing Accuracy**: **49%**\n\n- **Overfit**\n  - Model has memorized everything\n  - **Training Accuracy**: **99%**\n  - **Testing Accuracy**: **46%**\n\n- **Goodfit**\n  - Model has performed well on the testing data as well alongwith the training data \n  - **Training Accuracy**: **93%**\n  - **Testing Accuracy**: **91%**\n\n","metadata":{"id":"09FhiyffNSKh"}},{"cell_type":"markdown","source":"Now, let's use a Random Forest Classifier to overcome the variance problem to get a better generalizable result","metadata":{"id":"U45LF_4-IgOG"}},{"cell_type":"markdown","source":"###**Build an Interactive Random Forest Model**","metadata":{"id":"kG_ceYqnTOVi"}},{"cell_type":"code","source":"@interact\ndef plot_tree_rf(crit= ['gini','entropy'],\n                 bootstrap= ['True', 'False'],\n                 depth=IntSlider(min= 1 ,max= 20,value=3, continuous_update=False),\n                 forests=IntSlider(min= 1,max= 1000,value= 100,continuous_update=False),\n                 min_split=IntSlider(min= 2,max= 5,value= 2, continuous_update=False),\n                 min_leaf=IntSlider(min= 1,max= 5,value= 1, continuous_update=False)):\n  \n  estimator = RandomForestClassifier(\n      random_state = 1,\n      criterion = crit,\n      bootstrap = bootstrap,\n      n_estimators = forests,\n      max_depth = depth, \n      min_samples_split = min_split,\n      min_samples_leaf = min_leaf,\n      n_jobs = -1,\n      verbose = False)\n  \n  estimator.fit(X_train, y_train)\n\n  print('Random Forest Training Accuracy:', accuracy_score(y_train, estimator.predict(X_train)))\n  print('Random Forest Testing Accuracy:', accuracy_score(y_test, estimator.predict(X_test)))  \n\n  a = accuracy_score(y_train, estimator.predict(X_train))\n  b = accuracy_score(y_test, estimator.predict(X_test))\n\n  if a > 0.99:\n    print('Random Forest Training Accuracy',a, 'Random Forest Testing Accuracy', b)\n    print('Criterion:',crit,'\\n', 'Bootstrap:', bootstrap,'\\n', 'Depth:', depth,'\\n', 'forests:', forests,'\\n', 'Min_split:', min_split,'\\n', 'Min_leaf:', min_leaf,'\\n')\n","metadata":{"id":"bPNGM1JwBrbN","outputId":"61e012a1-c239-4a70-b022-b88d6b9dc059"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Advantages** & **Disadvantages** of Random Forest:\n\n**Advantages:**\n- Not prone overfitting\n- Runs efficiently huge data sets\n- Gives better accuracy than other classification algorithms\n\n**Disadvantages:**\n- Computationally Slower\n- Found to be biased while dealing with categorical variables\n- Although much lower than decision trees, overfitting is still a risk with random forests\n\n","metadata":{"id":"HNyMfpjAPrpH"}},{"cell_type":"markdown","source":"###**Implement GridSearchCV and RandomizedSearchCV Model**","metadata":{"id":"GR9BXt2vxl13"}},{"cell_type":"markdown","source":"**GridSearchCV**","metadata":{"id":"5A-OhRnTC0GN"}},{"cell_type":"code","source":"#GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\nrfc=RandomForestClassifier(random_state=42)\n\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nCV_rfc.fit(X_train, y_train)\n\n","metadata":{"id":"3EE8M9aYxkKb","outputId":"c93eda0d-4ab9-46fa-ba34-6b931f97a276"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CV_rfc.best_params_","metadata":{"id":"2CyQH60A0az7","outputId":"0fb571fd-18e7-4fd4-dd89-e338e5a3a44d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc1=RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 200, max_depth=8, criterion='gini')","metadata":{"id":"1v0aAz350bSS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc1.fit(X_train, y_train)","metadata":{"id":"mtnuMHYK0bIK","outputId":"cbf7b1a2-7f74-4b81-db8b-672201e118a2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred=rfc1.predict(X_test)","metadata":{"id":"k9S3Ez2A0apI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy for Random Forest on CV data: \",accuracy_score(y_test,pred))","metadata":{"id":"z5XLJU164YuM","outputId":"a1416ff3-d156-484e-d18f-6e2beb893d2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\ngd_roc=roc_auc_score(y_test, pred)\ngd_acc = accuracy_score(y_test, pred)\ngd_prec = precision_score(y_test, pred)\ngd_rec = recall_score(y_test, pred)\ngd_f1 = f1_score(y_test, pred)","metadata":{"id":"X7rGWSlscrFJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**RandomizedSearchCV**","metadata":{"id":"rn5cpB2yCweJ"}},{"cell_type":"code","source":"#Randomized Search CV\n\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","metadata":{"id":"f8hkurSf4YeU","outputId":"233b5fca-37a4-42d2-bb34-3cd1a85d6ffd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Base Model\nrfc=RandomForestClassifier(random_state=42)\n\nrf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)\n","metadata":{"id":"ddvoiu5g5ICl","outputId":"5bc4fd98-8b7d-4214-e812-65dd5008433e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_random.best_params_","metadata":{"id":"iiYx605W5IMQ","outputId":"436ee49c-2017-4d33-8895-999f69e98429"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc1=RandomForestClassifier(random_state=42, bootstrap = False,\n max_depth = 70,\n max_features= 'sqrt',\n min_samples_leaf = 1,\n min_samples_split= 5,\n n_estimators= 1600)\n","metadata":{"id":"o9_FbbA05H6W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc1.fit(X_train, y_train)","metadata":{"id":"xuisUdN7BaCH","outputId":"38f3ee92-d1c2-433e-c3ab-616bbd14c77a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred=rfc1.predict(X_test)","metadata":{"id":"iOJ9rmEcBaVk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###**Model Evaluation**","metadata":{"id":"hBVdy_1RS16q"}},{"cell_type":"markdown","source":"**Accuracy:** No. of correct predictions made by the model over all kinds predictions made\n\n**When to use Accuracy:**\n\nAccuracy is a good measure when the target variable classes in the data are nearly balanced. For example, No. of people who Survived Titanic (60% yes - 40% no)","metadata":{"id":"MiNq7GkMS4oo"}},{"cell_type":"code","source":"print(\"Accuracy: \",accuracy_score(y_test,pred))","metadata":{"id":"93ieKVQRBZ4E","outputId":"03fe2f84-6857-428b-c5f2-b9cf385f1d64"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix:**\n Gives the Performance of a classification model on a set of test data for which the true values are known.\n\nA way to visualize **Precision** and **Recall**\n\n**When to use Confusion Matrix:** When we have an Imbalanced Classification Task\n\n","metadata":{"id":"Ch1IdTmOS7d0"}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, pred)","metadata":{"id":"g_VszRYiTYqd","outputId":"fdd51841-59e8-463c-bb07-3f4fdb39e1dc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Precision:**\n  - What percebtage of positive predictions made were correct? This is **Precision**\n  - No. of True Positives divided by the no. of True Positives plus the No. of False Positives\n \n- **Recall:** Ratio of True Positives to all the positives in your Dataset\n\n- **When to use Precision & Recall:** \n - In the credit card fraud detection task, lets say we modify the model slightly, and identify a single transaction correctly as fraud. \n\n - Now, our precision will be 1.0 (no false positives) but our recall will be very low because we will still have many false negatives. \n\n - If we go to the other extreme and classify all transactions as fraud, we will have a recall of 1.0 — we’ll catch every fraud transaction — but our precision will be very low and we’ll misclassify many legit transactions. In other words, as we increase precision we decrease recall and vice-versa.\n\n- **F1-Score:**\n F1 Score is the weighted average of Precision and Recall. F1 is usually more useful than accuracy, especially when we have an uneven class distribution\n\n - **When to use F1-Score:** \n   - Useful when you have data with imbalance classes\n   - Let us say, we have a model with a precision of 1, and recall of 0 which gives a simple average as 0.5 and an F1 score of 0\n   - If one of the parameters is low, the second one no longer matters in the F1 score \n   - The F1 score favors classifiers that have similar precision and recall\n   - F1 score is a better measure to use if you are seeking a balance between Precision and Recall\n","metadata":{"id":"GmODS-WEUXya"}},{"cell_type":"code","source":"from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n\nroc=roc_auc_score(y_test, pred)\nacc = accuracy_score(y_test, pred)\nprec = precision_score(y_test, pred)\nrec = recall_score(y_test, pred)\nf1 = f1_score(y_test, pred)\n\nresults = pd.DataFrame([['RandomizedSearchCV', acc,prec,rec, f1,roc], \n                        ['GridSearchCV',gd_acc, gd_prec, gd_rec, gd_f1, gd_roc]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults","metadata":{"id":"TRbJSWegUbzV","outputId":"7bcfce8e-8640-47f3-ee35-e26711766081"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Saving the model and dumping it to a pickle file**","metadata":{"id":"g9eS3wZUslxN"}},{"cell_type":"code","source":"import pickle \n\nfilename = 'final_model.sav'\npickle.dump(rfc1, open(filename, 'wb'))\n ","metadata":{"id":"rJbMRq0yslJ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###**Interpreting Employee Attrition Prediction With SHAP**\n","metadata":{"id":"zQogqLRyEuuF"}},{"cell_type":"markdown","source":"**SHAP** (SHapley Additive exPlanations) :break down a prediction to show the impact of each feature\n\n**Install SHAP**: pip install shap","metadata":{"id":"C8GhuHKDlmBd"}},{"cell_type":"code","source":"pip install shap","metadata":{"id":"scKGskzgoAUV","outputId":"797848ba-6861-46c3-9e5e-2bb9cb3ad41a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap","metadata":{"id":"kwuyLwaen9zQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**shap.summary_plot function**\n\n- Produces the variable importance plot\n- A variable importance plot lists the most significant variables in descending order\n- The top variables contribute more to the model than the bottom ones and thus have high predictive power","metadata":{"id":"gtdjv3mjqyhY"}},{"cell_type":"code","source":"\nshap_values = shap.TreeExplainer(rfc1).shap_values(X_train)\nshap.summary_plot(shap_values, X_train, plot_type=\"bar\")","metadata":{"id":"RL3gN5CGqdV2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dependency Plot**\n\n- Shows the effect a single feature has on the prediction\n- How much the prediction depends on a particular feature\n- shap.dependence_plot(indexoffeature,matrix_shap_values,dataset_matrix)","metadata":{"id":"PUW2z8-dvDOZ"}},{"cell_type":"code","source":"shap.dependence_plot('satisfaction', shap_values, X_train)","metadata":{"id":"Eq4VDkPttFFy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###**PyCaret**\n\n\nUse **PyCaret** to find the best model and perform Automatic Hyperparameter tuning\n\n**NOTE:** It is alwasy used in Industry as a Directional Tool","metadata":{"id":"QIMMxLRhD4jx"}},{"cell_type":"markdown","source":"**PyCaret** is an open source, low-code machine learning library in **Python** that allows you to go from preparing your data to deploying your model within minutes in your choice of notebook environment\n\n[**Click Here!**](https://pycaret.org/) to learn more about **PyCaret**\n\n**Installing PyCaret**\n\n- !pip install pycaret","metadata":{"id":"I3g8pRpPFTX4"}},{"cell_type":"markdown","source":"####**Tasks to be performed**\n\n- Import PyCaret and load the data set\n- Initialize or setup the environment \n- Compare Multiple Models and their Accuracy Metrics\n- Create the model\n- Tune the model\n- Evaluate the model\n","metadata":{"id":"fowOU43GJALe"}},{"cell_type":"markdown","source":"####**Import PyCaret and load the data set**","metadata":{"id":"noukY5_MJYrA"}},{"cell_type":"code","source":"!pip install pycaret","metadata":{"id":"n0EgBOA-_pjf","outputId":"18c27cb8-83a0-4985-9a36-071f6a4e06dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pycaret.classification as pc\n#dir(pc)","metadata":{"id":"70l6dakLBq2i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading the dataset\nimport pandas as pd\ndf = pd.read_csv('/content/employee_data.csv?dl=0')\n\ndf.head() #Printing the first 5 rows of dataframe","metadata":{"id":"72pPfK-sIqw-","outputId":"d4bc96f1-af70-460e-d60d-f24bf820fd35"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['department'].unique()","metadata":{"id":"OONiQcT_13k8","outputId":"9c40547e-523f-4eaf-9c44-9d9a9b232412"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####**Initialize or setup the environment**","metadata":{"id":"QQsWYckLJd7B"}},{"cell_type":"code","source":"pc.setup(df, target='quit')","metadata":{"id":"pRt5YV8z7XP0","outputId":"5d67811c-1643-4bd3-99f4-8981f88e953a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n**Observations:**\n- The target type (Serial No. 2) is **Binary** because we have two values in **quit** column i.e., **0** and **1**\n- The data contains **3** Numeric Features and **6** Categorical Features\n___","metadata":{"id":"TeGGECIHJgox"}},{"cell_type":"markdown","source":"####**Compare Multiple Models and their Accuracy Metrics**","metadata":{"id":"OmJIvISnK1II"}},{"cell_type":"code","source":"pc.compare_models()","metadata":{"id":"elmNrJMbBs2j","outputId":"af3923be-9f9e-4544-e404-70abbd3a03d8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:** Don't worry about the models. You are gonna learn most of them in the coming modules","metadata":{"id":"39Tc1MbWO_er"}},{"cell_type":"markdown","source":"####**Create the Model**\n\n","metadata":{"id":"jN3felJtL7A0"}},{"cell_type":"code","source":"rf_model = pc.create_model('rf') #Performs K-Fold (10) CV for the selected model","metadata":{"id":"cCHcnI5PBsjE","outputId":"2fa21c70-ace0-4ca4-b264-dc3a5fffec6e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####**Tune the Model**","metadata":{"id":"EceiM3QyMzSi"}},{"cell_type":"code","source":"tuned_rf = pc.tune_model(rf_model)","metadata":{"id":"PqKgELtvBsYI","outputId":"f6f12acb-f568-4fb2-be09-41e8dc853a9a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rf_model)","metadata":{"id":"JPYTodCMM_Le","outputId":"726cb82c-2a3c-4ba4-f837-aed5b3bb4bd8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tuned_rf)","metadata":{"id":"2HWomCUeM_aR","outputId":"d85adaa6-a792-41f9-bfb7-dc2a05e0b176"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See the difference between the original model (**rf_model**) and the tuned model (**tuned_rf**)","metadata":{"id":"6w3qt9rkOjbE"}},{"cell_type":"markdown","source":"####**Evaluate the Model**","metadata":{"id":"JlLakBVMOHce"}},{"cell_type":"code","source":"tuned_rf_eval = pc.evaluate_model(tuned_rf)","metadata":{"id":"jGlnn50KM_Ru","outputId":"6ee11fd0-e53b-49a2-8af9-fa10f1b73675"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###**Deploying the Model Using Streamlit**\n\nGo to your local system and use a text editor such as **Sublime Text** to deploy your app using the pickle file genereated","metadata":{"id":"t6LPiwIgCm7x"}},{"cell_type":"markdown","source":"- **Save the next cell as a .py file**\n- **Run it in your local system** (streamlit run filename.py)","metadata":{"id":"VRzMyVReQTXi"}},{"cell_type":"code","source":"from pycaret.classification import load_model, predict_model\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\n\n#Loading Trained Model\nmodel = load_model('Final_model')\n\n\ndef predict(model, input_df):\n    predictions_df = predict_model(estimator=model, data=input_df)\n    #predict_model function takes a trained model object and the dataset to predict\n    \n    predictions = predictions_df['Label'][0]\n    return predictions\n\n\nst.title('Employee Attrition Prediction Web App')\n\nsatisfaction_level=st.number_input('satisfaction_level' , min_value=0.1, max_value=1.0, value=0.1)\nlast_evaluation =st.number_input('last_evaluation',min_value=0.1, max_value=1.0, value=0.1)\nnumber_project = st.number_input('number_project', min_value=0, max_value=50, value=5)\ntime_spend_company = st.number_input('time_spend_company', min_value=1, max_value=10, value=3)\nWork_accident = st.number_input('Work_accident',  min_value=0, max_value=50, value=0)\npromotion_last_5years = st.number_input('promotion_last_5years',  min_value=0, max_value=50, value=0)\nsalary = st.selectbox('salary', ['low', 'high','medium'])\naverage_montly_hours = st.number_input('average_montly_hours',  min_value=96, max_value=310, value=100)\ndepartment = st.selectbox('department', ['sales', 'accounting', 'hr', 'technical', 'support', 'management',\n       'IT', 'product_mng', 'marketing', 'RandD'])\n\n\noutput=\"\"\n\ninput_dict={'satisfaction_level':satisfaction_level,'last_evaluation':last_evaluation,'number_project':number_project,'time_spend_company':time_spend_company,'Work_accident': Work_accident,'promotion_last_5years':promotion_last_5years,'salary' : salary, 'average_montly_hours':average_montly_hours, 'department':department}\n\n\ninput_df = pd.DataFrame([input_dict])\nprint(input_df)\n\nif st.button(\"Predict\"):\n\toutput = predict(model=model, input_df=input_df)\n\toutput = str(output)\n        \nif output=='1':\n\tst.success('Employee will leave the company')\nelse:\n\tst.success('Employee will stay')","metadata":{"id":"3qw0PDkekJCg"},"execution_count":null,"outputs":[]}]}